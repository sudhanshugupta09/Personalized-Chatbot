{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for dialogue system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# preprocessed data\n",
    "# from datasets.cornell_corpus import data\n",
    "# import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read movie lines\n",
    "\n",
    "lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "\n",
    "dialogue_lines = {}  # Dictionary --> Key= Line Number , Value = Dialogues\n",
    "\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        dialogue_lines[_line[0]] = _line[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304713"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dialogue_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['L194', 'L195', 'L196', 'L197'], ['L198', 'L199'], ['L200', 'L201', 'L202', 'L203'], ['L204', 'L205', 'L206'], ['L207', 'L208'], ['L271', 'L272', 'L273', 'L274', 'L275'], ['L276', 'L277'], ['L280', 'L281'], ['L363', 'L364'], ['L365', 'L366']]\n"
     ]
    }
   ],
   "source": [
    "# get conversation blocks\n",
    "\n",
    "conversation_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "conversations = [ ]\n",
    "for line in conversation_lines[:-1]:\n",
    "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    conversations.append(_line.split(','))\n",
    "\n",
    "print(conversations[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83097"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_len=np.shape(conversations)[0]\n",
    "conversations_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L194 Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "L195 Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "L196 Not the hacking and gagging and spitting part.  Please.\n",
      "L197 Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "L198 You're asking me out.  That's so cute. What's your name again?\n",
      "L199 Forget it.\n",
      "L200 No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "L201 Cameron.\n",
      "L202 The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "L203 Seems like she could get a date easy enough...\n",
      "L204 Why?\n",
      "L205 Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
      "L206 That's a shame.\n",
      "L207 Gosh, if only we could find Kat a boyfriend...\n",
      "L208 Let me see what I can do.\n",
      "L271 C'esc ma tete. This is my head\n",
      "L272 Right.  See?  You're ready for the quiz.\n",
      "L273 I don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\n",
      "L274 That's because it's such a nice one.\n",
      "L275 Forget French.\n",
      "L276 How is our little Find the Wench A Date plan progressing?\n",
      "L277 Well, there's someone I think might be --\n",
      "L280 There.\n",
      "L281 Where?\n",
      "L363 You got something on your mind?\n",
      "L364 I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\n",
      "L365 You have my word.  As a gentleman\n",
      "L366 You're sweet.\n"
     ]
    }
   ],
   "source": [
    "## Print line Ids and the dialogues\n",
    "\n",
    "for i in range(10):\n",
    "    for line in conversations[i]:\n",
    "        if line in dialogue_lines:\n",
    "            print(line, dialogue_lines[line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "Not the hacking and gagging and spitting part.  Please.\n",
      "Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "You're asking me out.  That's so cute. What's your name again?\n",
      "Forget it.\n",
      "No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "Cameron.\n",
      "The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "Seems like she could get a date easy enough...\n",
      "Why?\n",
      "Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
      "That's a shame.\n",
      "Gosh, if only we could find Kat a boyfriend...\n",
      "Let me see what I can do.\n",
      "C'esc ma tete. This is my head\n",
      "Right.  See?  You're ready for the quiz.\n",
      "I don't want to know how to say that though.  I want to know useful things. Like where the good stores are.  How much does champagne cost?  Stuff like Chat.  I have never in my life had to point out my head to someone.\n",
      "That's because it's such a nice one.\n",
      "Forget French.\n",
      "How is our little Find the Wench A Date plan progressing?\n",
      "Well, there's someone I think might be --\n",
      "There.\n",
      "Where?\n",
      "You got something on your mind?\n",
      "I counted on you to help my cause. You and that thug are obviously failing. Aren't we ever going on our date?\n",
      "You have my word.  As a gentleman\n",
      "You're sweet.\n"
     ]
    }
   ],
   "source": [
    "## extract conversations // extract dialogues for the conversations\n",
    "counter = 0\n",
    "for conv in conversations[:10]:\n",
    "    for line_id in conv:\n",
    "        print(dialogue_lines[line_id])\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## gather dataset\n",
    "\n",
    "initiation = []; response = []\n",
    "\n",
    "for conv in conversations:\n",
    "    if len(conv) %2 != 0:\n",
    "        conv = conv[:-1]\n",
    "    for i in range(len(conv)):\n",
    "        if i%2 == 0:\n",
    "            initiation.append(dialogue_lines[conv[i]])\n",
    "        else:\n",
    "            response.append(dialogue_lines[conv[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to lower case \n",
    "initiation = [ line.lower() for line in initiation ]\n",
    "response = [ line.lower() for line in response ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<initiation>  can we make this quick?  roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad.  again. \n",
      " <response> well, i thought we'd start with pronunciation, if that's okay with you. \n",
      "\n",
      "<initiation>  not the hacking and gagging and spitting part.  please. \n",
      " <response> okay... then how 'bout we try out some french cuisine.  saturday?  night? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"<initiation> \", initiation[i], \"\\n\",\"<response>\", response[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21% filtered from original data\n",
      "fileterd q \n",
      " ['not the hacking and gagging and spitting part.  please.', \"you're asking me out.  that's so cute. what's your name again?\", \"no, no, it's my fault -- we didn't have a proper introduction ---\", 'gosh, if only we could find kat a boyfriend...', \"c'esc ma tete. this is my head\", 'how is our little find the wench a date plan progressing?', 'there.', 'you got something on your mind?', 'you have my word.  as a gentleman', 'how do you get your hair to look like that?']  \n",
      " \n",
      "filtered a [\"okay... then how 'bout we try out some french cuisine.  saturday?  night?\", 'forget it.', 'cameron.', 'let me see what i can do.', \"right.  see?  you're ready for the quiz.\", \"well, there's someone i think might be --\", 'where?', \"i counted on you to help my cause. you and that thug are obviously failing. aren't we ever going on our date?\", \"you're sweet.\", \"eber's deep conditioner every two days. and i never, ever use a blowdryer without the diffuser attachment.\"]\n"
     ]
    }
   ],
   "source": [
    "# Filter out short and long sentences\n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "dialogue_limits = {'initiation_max': 25,\n",
    "                   'initiation_min': 2,\n",
    "                   'response_max': 25,\n",
    "                   'response_min': 2\n",
    "                  }\n",
    "\n",
    "\n",
    "len_filtered_initiation = []\n",
    "len_filtered_response = []\n",
    "\n",
    "dataset_length = len(initiation)\n",
    "\n",
    "# check to have responses to every initiation\n",
    "assert len(initiation) == len(response)\n",
    "\n",
    "for i in range(dataset_length):\n",
    "    init_len, resp_len = len(tknzr.tokenize(initiation[i])), len(tknzr.tokenize(response[i]))\n",
    "    if init_len >= dialogue_limits['initiation_min'] and init_len <= dialogue_limits['initiation_max']:\n",
    "        if resp_len >= dialogue_limits['response_min'] and resp_len <= dialogue_limits['response_max']:\n",
    "            len_filtered_initiation.append(initiation[i])\n",
    "            len_filtered_response.append(response[i])\n",
    "\n",
    "# print the filtered data value\n",
    "filtered_data_len = len(len_filtered_initiation)\n",
    "filtered = int((dataset_length - filtered_data_len)*100/dataset_length)\n",
    "print(str(filtered) + '% filtered from original data')\n",
    "\n",
    "print(\"fileterd q \\n\" ,len_filtered_initiation[:10], \" \\n \\nfiltered a\", len_filtered_response[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(len_filtered_initiation) == len(len_filtered_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_initiation = []\n",
    "tokenized_response = []\n",
    "\n",
    "for init in len_filtered_initiation[:]:\n",
    "    tokenized_initiation.append(tknzr.tokenize(init))\n",
    "\n",
    "for resp in len_filtered_response[:]:\n",
    "    tokenized_response.append(tknzr.tokenize(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q : [['gigglepuss', 'is', 'playing', 'there', 'tomorrow', 'night', '.']]; \n",
      "a : [[\"don't\", 'make', 'me', 'do', 'it', ',', 'man']]\n",
      "\n",
      "q : [['cameron', ',', \"i'm\", 'a', 'little', 'busy']]; \n",
      "a : [[\"it's\", 'off', '.', 'the', 'whole', 'thing', '.']]\n",
      "\n",
      "q : [['what', \"'\", 're', 'you', 'talking', 'about', '?']]; \n",
      "a : [[\"she's\", 'partial', 'to', 'joey', ',', 'not', 'me']]\n",
      "\n",
      "q : [[\"what'd\", 'you', 'do', 'to', 'her', '?']]; \n",
      "a : [['i', 'don', \"'\", 't', 'know', '.', 'i', 'decided', 'not', 'to', 'nail', 'her', 'when', 'she', 'was', 'too', 'drunk', 'to', 'remember', 'it', '.']]\n",
      "\n",
      "q : [['she', 'hates', 'you', 'with', 'the', 'fire', 'of', 'a', 'thousand', 'suns', '.', \"that's\", 'a', 'direct', 'quote']]; \n",
      "a : [['she', 'just', 'needs', 'time', 'to', 'cool', 'off', \"i'll\", 'give', 'it', 'a', 'day', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tokenize every initiation and the response\n",
    "\n",
    "for init,resp in zip(tokenized_initiation[100:105], tokenized_response[100:105]):\n",
    "    print('q : [{0}]; \\na : [{1}]\\n'.format(init,resp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index 2 word and word to index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "UNK = 'unk'\n",
    "vocab_size = 8000\n",
    "\n",
    "# import pickle\n",
    "\n",
    "tokenized_sentences = tokenized_initiation + tokenized_response\n",
    "freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "# get vocabulary of 'vocab_size' most used words\n",
    "vocabulary = freq_dist.most_common(vocab_size)\n",
    "# index2word\n",
    "index2word = ['_'] + [UNK] + [ vocab[0] for vocab in vocabulary ]\n",
    "# word2index\n",
    "word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1% filtered from original data\n",
      "1744\n",
      "107043\n",
      "108787\n"
     ]
    }
   ],
   "source": [
    "## Filter out data based on the number of unknown tokens \n",
    "data_len = len(tokenized_initiation)\n",
    "\n",
    "unk_filtered_inititation = []\n",
    "unk_filtered_response = []\n",
    "\n",
    "\n",
    "for inits, resps in zip(tokenized_initiation, tokenized_response):\n",
    "    unk_count_init = len([ w for w in inits if w not in word2index ])\n",
    "    unk_count_resp = len([ w for w in resps if w not in word2index ])\n",
    "    if unk_count_resp <= 2:\n",
    "        if unk_count_init > 0:\n",
    "            if unk_count_init/len(inits) > 0.2:\n",
    "                pass\n",
    "        unk_filtered_inititation.append(inits)\n",
    "        unk_filtered_response.append(resps)\n",
    "\n",
    "# print the fraction of the original data, filtered\n",
    "filt_data_len = len(unk_filtered_inititation)\n",
    "filtered = int((data_len - filt_data_len)*100/data_len)\n",
    "print(str(filtered) + '% filtered from original data')\n",
    "print(data_len - filt_data_len)\n",
    "print(len(unk_filtered_inititation))\n",
    "print(data_len)\n",
    "# return filtered_q, filtered_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Final dataset len : 107043\n"
     ]
    }
   ],
   "source": [
    "print('\\n Final dataset len : ' + str(len(unk_filtered_inititation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_w2v(seq, lookup, maxlen):\n",
    "    indices = []\n",
    "    for word in seq:\n",
    "        if word in lookup:\n",
    "            indices.append(lookup[word])\n",
    "        else:\n",
    "            indices.append(lookup[UNK])\n",
    "    return indices + [0]*(maxlen - len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding zeros at the end of sentences and making them of equal length for the input in LSTM\n",
    "\n",
    "dataset_len = len(tokenized_initiation)\n",
    "\n",
    "\n",
    "final_initiations = np.zeros([dataset_len, dialogue_limits['initiation_max']], dtype=np.int32) \n",
    "final_responses = np.zeros([dataset_len, dialogue_limits['response_max']], dtype=np.int32)\n",
    "\n",
    "for i in range(dataset_len):\n",
    "    init_indices = padding_w2v(tokenized_initiation[i], word2index, dialogue_limits['initiation_max'])\n",
    "    resp_indices = padding_w2v(tokenized_response[i], word2index, dialogue_limits['response_max'])\n",
    "    final_initiations[i] = np.array(init_indices)\n",
    "    final_responses[i] = np.array(resp_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_initiations) == len(final_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   19,  653,   66,  294,  175,    2,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0],\n",
       "       [5014,    3,   24,    9,  127,  691,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0],\n",
       "       [  14,   50, 3034,    5,  223,   43,    4,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0],\n",
       "       [ 586,    5,   22,    8,   74,    4,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0],\n",
       "       [  75, 1747,    5,   40,    7,  534,   17,    9,  466,    1,    2,\n",
       "          55,    9, 2081, 4754,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_initiations[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  21,  129,   16,   22,   12,    3,  111,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0],\n",
       "       [  34,  134,    2,    7,  360,  143,    2,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0],\n",
       "       [ 149, 6076,    8, 1513,    3,   31,   16,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0],\n",
       "       [   6, 1302,   50, 1294,   26,    2,    6, 1069,   31,    8, 2495,\n",
       "          74,  103,   75,   37,  112,  875,    8,  225,   12,    2,    0,\n",
       "           0,    0,    0],\n",
       "       [  75,   39,  617,   90,    8,  552,  134,   80,  151,   12,    9,\n",
       "         231,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_responses[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% unknown : 3.5067504233579667\n",
      "Dataset count : 108787\n"
     ]
    }
   ],
   "source": [
    "## Print the unknown count and the total dataset count\n",
    "unk_count = (final_initiations == 1).sum() + (final_responses == 1).sum()\n",
    "word_count = (final_initiations > 1).sum() + (final_responses > 1).sum()\n",
    "\n",
    "print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
    "print('Dataset count : ' + str(final_initiations.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the arrays to disk for future imports\n",
    "np.save('idx_init.npy', final_initiations)\n",
    "np.save('idx_resp.npy', final_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare data for testing and training\n",
    "\n",
    "ratio = [0.70, 0.15, 0.15]\n",
    "data_len = len(final_initiations)\n",
    "\n",
    "train_ratio = int(0.70 * data_len)\n",
    "test_ratio = int(0.15 * data_len)\n",
    "valid_ratio = int(0.15 * data_len)\n",
    "\n",
    "lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "trainX = final_initiations[:train_ratio]\n",
    "trainY = final_responses[:train_ratio]\n",
    "\n",
    "testX = final_initiations[train_ratio : (train_ratio+test_ratio)]\n",
    "testY = final_responses[train_ratio : (train_ratio+test_ratio)]\n",
    "\n",
    "validX = final_initiations[:-valid_ratio]\n",
    "validY = final_responses[:-valid_ratio]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_seq_length = trainX.shape[-1]\n",
    "resp_seq_length = trainY.shape[-1]\n",
    "batch_size = 16\n",
    "init_vocab_size = len(index2word)  \n",
    "resp_vocab_size = init_vocab_size\n",
    "emb_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating batches to feed into LSTM\n",
    "\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "def batch(x, y, batch_size):\n",
    "    while True:\n",
    "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
    "        yield x[sample_idx].T, y[sample_idx].T\n",
    "\n",
    "validation_batch = batch(validX, validY, 32)\n",
    "test_batch = batch(testX, testY, 256)\n",
    "train_batch = batch(trainX, trainY, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<log> Building Graph </log>"
     ]
    }
   ],
   "source": [
    "## Building Tensorflow graph\n",
    "\n",
    "import seq2seq_wrapper\n",
    "\n",
    "model = seq2seq_wrapper.Seq2Seq(xseq_len=init_seq_length,\n",
    "                               yseq_len=resp_seq_length,\n",
    "                               xvocab_size=init_vocab_size,\n",
    "                               yvocab_size=resp_vocab_size,\n",
    "                               ckpt_path='ckpt/cornell_corpus/',\n",
    "                               emb_dim=emb_dim,\n",
    "                               num_layers=3\n",
    "                               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<log> Training started </log>\n",
      "Interrupted by user at iteration 41\n"
     ]
    }
   ],
   "source": [
    "sess = model.train(train_batch, validation_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 25)\n"
     ]
    }
   ],
   "source": [
    "input_ = test_batch.__next__()[0]\n",
    "output = model.predict(sess, input_)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence, lookup, separator=''):\n",
    "    return separator.join([ lookup[element] for element in sequence if element ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init : [bill id like you to meet jack torrance]; resp : [how much do you think]\n",
      "init : [and who are you to talk you were nothing you couldnt even sing i must have been out of my mind]; resp : [i dont know what youre talking about]\n",
      "init : [by breaking up a companys assets]; resp : [what are you talking about]\n",
      "init : [what is it]; resp : [i dont know]\n",
      "init : [ill see you there]; resp : [ill get out]\n",
      "init : [okay ill be talking to you]; resp : [youre not going to get out]\n",
      "init : [i must be outta my mind buddy inituit it]; resp : [okay for a minute]\n",
      "init : [when are you going to let the police know]; resp : [you dont know what youre talking about]\n",
      "init : [you can do it]; resp : [yeah i think so]\n",
      "init : [like hell you know if you fellows stuck together stead of letting them walk all over you they might not try it]; resp : [if you werent talking about me i dont know what youre talking about]\n",
      "init : [wait are you saying that i dont appreciate]; resp : [i know you know what i mean]\n",
      "init : [no you just looked as if you did]; resp : [well i didnt believe you]\n",
      "init : [youre terrible]; resp : [yeah but im not]\n",
      "init : [you would take a pain n the arse full stop]; resp : [dont worry about it]\n",
      "init : [yeah howd the kid hold up]; resp : [whats that]\n",
      "init : [yes well miss ratched feels that youre a disturbing influence on the other patients]; resp : [dont you tell me]\n",
      "init : [you leading a sinituad]; resp : [what is it]\n",
      "init : [nah i dont think so]; resp : [why not]\n",
      "init : [hes pissing in our faces again and were just taking it]; resp : [i cant]\n",
      "init : [my lady]; resp : [she said you were a little girl]\n",
      "init : [im sorry i dont know what else to say except im sorry]; resp : [why would you like to be]\n",
      "init : [mr brandon couldnt be here he might have left something for you what would it look like]; resp : [thats it i think i can get it out]\n",
      "init : [nobodys gonna get you now get inside]; resp : [i cant go back]\n",
      "init : [i just turned 25 i was 24 for a whole year]; resp : [well i dont think so]\n",
      "init : [make sure he doesnt leave]; resp : [you sure]\n",
      "init : [do we need him]; resp : [i dont know what to do]\n",
      "init : [youre terry unk arent you]; resp : [what are you doing here]\n",
      "init : [so lets go]; resp : [i dont think so]\n",
      "init : [ya owe me twentyfive bucks]; resp : [its a good man]\n",
      "init : [shall i leave]; resp : [of course]\n",
      "init : [unk unk probably asleep by now do you want him to see you like this]; resp : [no way i dont want to get out of here]\n",
      "init : [well i really think hes got a chance]; resp : [i know]\n",
      "init : [youd better be inituiet sandy]; resp : [shut up]\n",
      "init : [buddy that was pretty unk of you pushing me away like that just when it was interesting]; resp : [what about you]\n",
      "init : [jesus christ you scared the shit out of me]; resp : [whats going on]\n",
      "init : [well im sorry im really sorry ellie]; resp : [its okay]\n",
      "init : [youre not a man because of a job duff]; resp : [but i said that]\n",
      "init : [he didnt lose her he threw her away]; resp : [hes not dead]\n",
      "init : [you unk have the gotta]; resp : [what the hell are you talking about]\n",
      "init : [whoa whoa what do you expect them to say youre alan unk]; resp : [im sorry i dont know]\n",
      "init : [doc what can i tell ya]; resp : [go ahead]\n",
      "init : [my lady this play will end badly i will tell]; resp : [lets get out of here]\n",
      "init : [what if i said goodbye]; resp : [then why are you]\n",
      "init : [im going to miss you]; resp : [no youre not]\n"
     ]
    }
   ],
   "source": [
    "replies = []\n",
    "for m, n in zip(input_.T, output):\n",
    "    init = data_utils.decode(sequence=m, lookup=index2word, separator=' ')\n",
    "    decoded = data_utils.decode(sequence=n, lookup=index2word, separator=' ').split(' ')\n",
    "    if decoded.count('unk') == 0:\n",
    "        if decoded not in replies:\n",
    "            print('inti : [{0}]; resp : [{1}]'.format(init, ' '.join(decoded)))\n",
    "            replies.append(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####   <<<<<<<< End of CODE >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
